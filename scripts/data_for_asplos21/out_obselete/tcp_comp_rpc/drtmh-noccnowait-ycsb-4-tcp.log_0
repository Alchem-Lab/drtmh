NOCC started with program [/noccnowait-tcp]. at 24-07-2020 01:24:59
[bench_runner.cc:341] Use TCP port 8886
[0m[bench_runner.cc:363] use scale factor: 30; with total 10 threads.
[0m[view.h:48] Start with 2 backups.
[0m[view.cc:10] total 4 backups to assign
[0m[bank_main.cc:134] ycsb param:set len=10 write num=2tx hot90num_hot100num_accounts 100000
[0m[bank_main.cc:143] here100
[0m[Bank]: check workload 0, 0, 0, 0, 0, 0, 100
[util.cc:161] malloc_huge_pages: size = 10737418240huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[librdma] get device name mlx5_0, idx 0
[librdma] : Device 0 has 1 ports
poller bind address tcp://*:8886
[bench_runner.cc:144] [TCP] creating shared sockets
[0m[TCP] creating shared sockets for tcp://172.23.33.33:8886
[TCP] creating shared sockets for tcp://172.23.33.34:8886
[TCP] creating shared sockets for tcp://172.23.33.35:8886
[TCP] creating shared sockets for tcp://172.23.33.36:8886
[bench_runner.cc:157] Total logger area 0.00390625G.
[0m[bench_runner.cc:167] Total two-phase committer area 0.00195312G.
[0m[bench_runner.cc:176] add RDMA store size 6.34766G.
[0m[bench_runner.cc:185] [Mem] RDMA heap size 3.64452G.
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[Bank], total 12000000 accounts loaded
[bank_main.cc:344] check cv balance 46280
[0m[Runner] local db size: 275.789 MB
[Runner] DID NOT start server for receiving QP connection requests.
[Runner] Cache size: 0 MB
[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[Bank], total 12000000 accounts loaded
[bench_runner.cc:269] [Runner] Backup DB[0] for 2 size: 645.156 MB
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[Bank], total 12000000 accounts loaded
[bench_runner.cc:269] [Runner] Backup DB[1] for 3 size: 644.793 MB
[0mworker: 0 binding 0
worker: 1 binding 2
worker: 2 binding 4
worker: 3 binding 6
worker: 4 binding 8
worker: 5 binding 10
worker: 6 binding 12
worker: 7 binding 14
worker: 8 binding 16
worker: 9 binding 18
[Global sequence running] !
[NOCC] poller running!
[bench_listener2.cc:70] try log results to ./results//noccnowait-tcp_bank_4_10_10_100.log
[0m[bench_worker.h:156] Use RPC for logging.
[0m[bench_worker.h:192] Use two-sided RPC for two-phase-commit.
[0m[nowait_rdma.h:324] Use one-sided for read.
[0m[rworker.cc:47] Worker 11 on cpu 20with cor id 0
[0m[bench_listener2.cc:79] New monitor running!
[0m[bench_listener2.cc:130] All workers has initilized.
[0m[rworker.cc:47] Worker 12 on cpu 1with cor id 0
[0m[bench_listener2.cc:135] Starting Master Listener.
[0m[bench_listener2.cc:234] [LISTENER] receive start RPC.
[0m[rworker.cc:47] Worker 0 on cpu 0with cor id 0
[0m[rworker.cc:47] Worker 3 on cpu 6with cor id 0
[rworker.cc:47] Worker 2 on cpu 4with cor id 0
[0m[rworker.cc:47] Worker 1 on cpu 2with cor id 0
[0m[0m[rworker.cc:47] Worker 8 on cpu 16with cor id 0
[0m[rworker.cc:47] Worker 6 on cpu 12with cor id 0
[0m[rworker.cc:47] Worker 5 on cpu 10with cor id 0
[0m[rworker.cc:47] Worker 7 on cpu 14with cor id 0
[0m[rworker.cc:47] Worker 4 on cpu 8with cor id 0
[0m[rworker.cc:47] Worker 9 on cpu 18with cor id 0
[0mmerge data 11.905000 K from gorgon4
merge data 11.948000 K from gorgon6
merge data 11.688000 K from gorgon5
my throughput 11.559000 K, abort 692, commit 11559, abort ratio 0.056485
merge data 11.559000 K from gorgon3
[occ_statistics.h:65] lock lat: 1.14042e+06 ;counts 1221
[0m[occ_statistics.h:68] validate lat: 0 ;counts 1
[0m[occ_statistics.h:71] log lat: 1.89043e+06 ;counts 102
[0m[occ_statistics.h:74] twopc lat: 1.25408e+06 ;counts 119
[0m[occ_statistics.h:77] commit lat: 1.09759e+06 ;counts 119
[0m[occ_statistics.h:80] temp lat: 0 ;counts 1
[0m[occ_statistics.h:83] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:86] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:89] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:92] release_write lat: 1.67358e+06 ;counts 135
[0m@1 System throughput 47.100000 K, abort 0.050097
merge data 9.914000 K from gorgon6
merge data 9.923000 K from gorgon4
merge data 9.905000 K from gorgon5
my throughput 9.544000 K, abort 457, commit 9544, abort ratio 0.045695
merge data 9.544000 K from gorgon3
[occ_statistics.h:65] lock lat: 1.39403e+06 ;counts 958
[0m[occ_statistics.h:68] validate lat: 0 ;counts 1
[0m[occ_statistics.h:71] log lat: 2.16371e+06 ;counts 86
[0m[occ_statistics.h:74] twopc lat: 1.6615e+06 ;counts 93
[0m[occ_statistics.h:77] commit lat: 1.37664e+06 ;counts 93
[0m[occ_statistics.h:80] temp lat: 0 ;counts 1
[0m[occ_statistics.h:83] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:86] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:89] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:92] release_write lat: 2.43063e+06 ;counts 105
[0m@2 System throughput 39.286000 K, abort 0.072525
merge data 11.820000 K from gorgon5
merge data 12.145000 K from gorgon4
merge data 12.239000 K from gorgon6
my throughput 12.131000 K, abort 736, commit 12131, abort ratio 0.057201
merge data 12.131000 K from gorgon3
[occ_statistics.h:65] lock lat: 1.09312e+06 ;counts 1213
[0m[occ_statistics.h:68] validate lat: 0 ;counts 1
[0m[occ_statistics.h:71] log lat: 1.62225e+06 ;counts 112
[0m[occ_statistics.h:74] twopc lat: 1.35867e+06 ;counts 121
[0m[occ_statistics.h:77] commit lat: 1.53149e+06 ;counts 121
[0m[occ_statistics.h:80] temp lat: 0 ;counts 1
[0m[occ_statistics.h:83] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:86] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:89] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:92] release_write lat: 1.79399e+06 ;counts 123
[0m@3 System throughput 48.335000 K, abort 0.056141
[bench_listener2.cc:276] start to exit.
[0mstats for worker 0:
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
[NOCC] poller exit!
exiting master routine.
exiting master routine.
[bench_listener2.cc:295] Medium latency 8.10269ms, 90th latency 11.8356ms, 99th latency 29.5754ms; average latency: 8.997ms.
[0mYCSB executed 3333, latency: 8.998162, rw_size -nan, m 8.102704, 90 11.835573, 99 29.575435
succs ratio 0.945377
[bank_worker.h:126] worker exit.
[0m[bank_worker.h:129] read time: 0ms
[0m[occ_statistics.h:42] read_lat time: 0ms
[0m[occ_statistics.h:43] lock time: 0.525689ms
[0m[occ_statistics.h:44] release_write time: 0.854736ms
[0m[occ_statistics.h:45] release_read time: 0ms
[0m[occ_statistics.h:46] renew_lease time: 0ms
[0m[occ_statistics.h:47] validate time: 0ms
[0m[occ_statistics.h:48] log time: 0.822593ms
[0m[occ_statistics.h:49] 2pc time: 0.619402ms
[0m[occ_statistics.h:50] commit time: 0.580487ms
[0m[occ_statistics.h:52] 0
[0m[occ_statistics.h:53] 0.525689
[0m[occ_statistics.h:54] 0.854736
[0m[occ_statistics.h:55] 0
[0m[occ_statistics.h:56] 0.580487
[0m[occ_statistics.h:59] temp time: 0ms
[0m[bench_worker.cc:392] 0: 0
[0m[bench_worker.cc:392] 1: 155
[0m[bench_worker.cc:392] 2: 27
[0m[bench_worker.cc:392] 3: 0
[0m[bench_worker.cc:392] 4: 171
[0m[bench_worker.cc:392] 5: 0
[0m[bench_worker.cc:392] 6: 0
[0m[bench_worker.cc:392] 7: 0
[0m[bench_worker.cc:392] 8: 0
[0m[bench_worker.cc:392] 9: 0
[0m[bench_worker.cc:392] 10: 0
[0m[bench_worker.cc:392] 11: 0
[0m[bench_worker.cc:392] 12: 0
[0m[bench_worker.cc:392] 13: 0
[0m[bench_worker.cc:392] 14: 0
[0m[bench_worker.cc:392] 15: 0
[0m[bench_worker.cc:392] 16: 0
[0m[bench_worker.cc:392] 17: 0
[0m[bench_worker.cc:392] 18: 49891
[0m[bench_worker.cc:392] 19: 26987
[0m[bench_worker.cc:392] 20: 0
[0m[bench_worker.cc:392] 21: 0
[0m[bench_worker.cc:392] 22: 0
[0m[bench_worker.cc:392] 23: 0
[0m[bench_worker.cc:392] 24: 0
[0m[bench_worker.cc:392] 25: 0
[0m[bench_worker.cc:392] 26: 0
[0m[bench_worker.cc:392] 27: 3323
[0m[bench_worker.cc:392] 28: 0
[0m[bench_worker.cc:392] 29: 0
[0m[bench_worker.cc:392] 30: 0
[0m[bench_worker.cc:392] 31: 0
[0m[bench_worker.cc:392] 32: 0
[0m[bench_worker.cc:392] 33: 0
[0m[bench_worker.cc:392] 34: 0
[0m[bench_worker.cc:392] 35: 0
[0m[bench_worker.cc:392] 36: 0
[0m[bench_worker.cc:392] 37: 0
[0m[bench_worker.cc:392] 38: 0
[0m[bench_worker.cc:392] 39: 0
[0mmaster routine exit...
[bench_listener2.cc:307] benchmark ends
[0m[bench_runner.cc:313] main runner ends.
[0m