NOCC started with program [/noccwaitdie-tcp]. at 19-08-2020 08:17:28
[bench_runner.cc:341] Use TCP port 8892
[0m[bench_runner.cc:363] use scale factor: 30; with total 10 threads.
[0m[view.h:48] Start with 2 backups.
[0m[view.cc:10] total 4 backups to assign
[0m[bank_main.cc:134] ycsb param:set len=10 write num=2tx hot90num_hot100num_accounts 100000
[0m[bank_main.cc:143] here0
[0m[Bank]: check workload 17, 17, 17, 16, 17, 16, 0
[util.cc:161] malloc_huge_pages: size = 13883146240huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 12.9316
[0m[librdma] get device name mlx5_0, idx 0
[librdma] : Device 0 has 1 ports
poller bind address tcp://*:8892
[bench_runner.cc:144] [TCP] creating shared sockets
[0m[TCP] creating shared sockets for tcp://172.23.33.33:8892
[TCP] creating shared sockets for tcp://172.23.33.34:8892
[TCP] creating shared sockets for tcp://172.23.33.35:8892
[TCP] creating shared sockets for tcp://172.23.33.32:8892
[bench_runner.cc:157] Total logger area 0.00390625G.
[0m[bench_runner.cc:167] Total two-phase committer area 0.00195312G.
[0m[bench_runner.cc:176] add RDMA store size 7.8125G.
[0m[bench_runner.cc:185] [Mem] RDMA heap size 5.10934G.
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 0.294922
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 0.294922
[0m[util.cc:161] malloc_huge_pages: size = 314572800huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 0.294922
[0m[Bank], total 12000000 accounts loaded
[bank_main.cc:344] check cv balance 46280
[0m[Runner] local db size: 276.32 MB
[Runner] DID NOT start server for receiving QP connection requests.
[Runner] Cache size: 0 MB
[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 1.34375
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 1.34375
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 1.34375
[0m[Bank], total 12000000 accounts loaded
[bench_runner.cc:269] [Runner] Backup DB[0] for 2 size: 644.828 MB
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:178] huge page real size 1.34375
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[util.cc:161] malloc_huge_pages: size = 1440000000huge_page_sz = 2097152flag = 1
[0m[util.cc:175] huge page alloc failed!
[0m[Bank], total 12000000 accounts loaded
[bench_runner.cc:269] [Runner] Backup DB[1] for 3 size: 645.047 MB
[0mworker: 0 binding 0
worker: 1 binding 2
worker: 2 binding 4
worker: 3 binding 6
worker: 4 binding 8
worker: 5 binding 10
worker: 6 binding 12
worker: 7 binding 14
worker: 8 binding 16
worker: 9 binding 18
[Global sequence running] !
[NOCC] poller running!
[bench_listener2.cc:70] try log results to ./results//noccwaitdie-tcp_bank_4_10_17_100.log
[0m[bench_worker.h:156] Use RPC for logging.
[0mWAITDIE uses RPC LOCK.
WAITDIE uses RPC LOG.
WAITDIE uses RPC RELEASE.
WAITDIE uses RPC COMMIT.
[rworker.cc:47] Worker 11 on cpu 22with cor id 0
[0m[bench_listener2.cc:79] New monitor running!
[0m[bench_listener2.cc:130] All workers has initilized.
[0m[rworker.cc:47] Worker 12 on cpu 17with cor id 0
[0m[bench_listener2.cc:135] Starting Master Listener.
[0m[bench_listener2.cc:234] [LISTENER] receive start RPC.
[0m[rworker.cc:47] Worker 0 on cpu 0with cor id 0
[rworker.cc:47] Worker 3 on cpu 6with cor id 0
[0m[rworker.cc:47] Worker 2 on cpu 4with cor id 0
[0m[rworker.cc:47] Worker 8 on cpu 16with cor id 0
[0m[rworker.cc:47] Worker 7 on cpu 14with cor id 0
[0m[rworker.cc:47] Worker 6 on cpu 12with cor id 0
[0m[rworker.cc:47] Worker 4 on cpu 8with cor id 0
[0m[0m[rworker.cc:47] Worker 5 on cpu 10with cor id 0
[0m[rworker.cc:47] Worker 9 on cpu 18with cor id 0
[0m[rworker.cc:47] Worker 1 on cpu 2with cor id 0
[0mmerge data 47.087000 K from gorgon5
merge data 42.377000 K from gorgon4
merge data 43.022000 K from gorgon2
my throughput 43.918000 K, abort 11, commit 43918, abort ratio 0.000250
merge data 43.918000 K from gorgon3
[occ_statistics.h:111] lock lat: 1.2646e+06 ;counts 487
[0m[occ_statistics.h:114] lock cpu cycles: 16.5421 ;counts 487
[0m[occ_statistics.h:117] validate lat: 0 ;counts 1
[0m[occ_statistics.h:120] validate cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:123] log lat: 3.11297e+06 ;counts 214
[0m[occ_statistics.h:126] log cpu cycles: 133.486 ;counts 214
[0m[occ_statistics.h:129] twopc lat: 0 ;counts 1
[0m[occ_statistics.h:132] twopc cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:135] commit lat: 1.04704e+06 ;counts 258
[0m[occ_statistics.h:138] commit cpu cycles: 0.139535 ;counts 258
[0m[occ_statistics.h:141] temp lat: 0 ;counts 1
[0m[occ_statistics.h:144] temp cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:147] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:150] read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:153] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:156] renew_lease cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:159] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:162] release_read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:165] release_write lat: 372887 ;counts 258
[0m[occ_statistics.h:168] release_write cpu cycles: 12.0271 ;counts 258
[0m@1 System throughput 176.404000 K, abort 0.000861
merge data 60.503000 K from gorgon5
merge data 62.316000 K from gorgon4
merge data 60.615000 K from gorgon2
my throughput 58.679000 K, abort 28, commit 58679, abort ratio 0.000477
merge data 58.679000 K from gorgon3
[occ_statistics.h:111] lock lat: 1.16115e+06 ;counts 637
[0m[occ_statistics.h:114] lock cpu cycles: 2.89588e+16 ;counts 637
[0m[occ_statistics.h:117] validate lat: 0 ;counts 1
[0m[occ_statistics.h:120] validate cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:123] log lat: 1.80937e+06 ;counts 281
[0m[occ_statistics.h:126] log cpu cycles: 6.56468e+16 ;counts 281
[0m[occ_statistics.h:129] twopc lat: 0 ;counts 1
[0m[occ_statistics.h:132] twopc cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:135] commit lat: 1.11989e+06 ;counts 347
[0m[occ_statistics.h:138] commit cpu cycles: 18.4207 ;counts 347
[0m[occ_statistics.h:141] temp lat: 0 ;counts 1
[0m[occ_statistics.h:144] temp cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:147] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:150] read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:153] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:156] renew_lease cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:159] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:162] release_read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:165] release_write lat: 421783 ;counts 347
[0m[occ_statistics.h:168] release_write cpu cycles: 5.31606e+16 ;counts 347
[0m@2 System throughput 242.113000 K, abort 0.002143
merge data 49.801000 K from gorgon5
merge data 53.788000 K from gorgon4
merge data 53.568000 K from gorgon2
my throughput 50.939000 K, abort 37, commit 50939, abort ratio 0.000726
merge data 50.939000 K from gorgon3
[occ_statistics.h:111] lock lat: 1.29094e+06 ;counts 547
[0m[occ_statistics.h:114] lock cpu cycles: 3.75137 ;counts 547
[0m[occ_statistics.h:117] validate lat: 0 ;counts 1
[0m[occ_statistics.h:120] validate cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:123] log lat: 2.26535e+06 ;counts 252
[0m[occ_statistics.h:126] log cpu cycles: 7.32014e+16 ;counts 252
[0m[occ_statistics.h:129] twopc lat: 0 ;counts 1
[0m[occ_statistics.h:132] twopc cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:135] commit lat: 1.21162e+06 ;counts 296
[0m[occ_statistics.h:138] commit cpu cycles: 37.8953 ;counts 296
[0m[occ_statistics.h:141] temp lat: 0 ;counts 1
[0m[occ_statistics.h:144] temp cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:147] read_lat lat: 0 ;counts 1
[0m[occ_statistics.h:150] read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:153] renew_lease lat: 0 ;counts 1
[0m[occ_statistics.h:156] renew_lease cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:159] release_read lat: 0 ;counts 1
[0m[occ_statistics.h:162] release_read cpu cycles: 0 ;counts 1
[0m[occ_statistics.h:165] release_write lat: 383352 ;counts 296
[0m[occ_statistics.h:168] release_write cpu cycles: 0.148649 ;counts 296
[0m@3 System throughput 208.096000 K, abort 0.002096
[bench_listener2.cc:276] start to exit.
[0mstats for worker 0:
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
exiting master routine.
[NOCC] poller exit!
exiting master routine.
exiting master routine.
SendPayment executed 2683, latency: 3.765825, rw_size -nan, m 3.063366, 90 4.471352, 99 20.136775
DepositChecking executed 2521, latency: 3.239495, rw_size -nan, m 2.473693, 90 3.345331, 99 21.265392
Balance executed 2531, latency: 1.446074, rw_size -nan, m 1.612186, 90 2.240995, 99 2.966635
Transact saving executed 2448, latency: 2.903473, rw_size -nan, m 2.422112, 90 3.269743, 99 17.775020
Write check executed 2676, latency: 3.858473, rw_size -nan, m 3.599217, 90 4.600844, 99 17.593273
Txn amal executed 2462, latency: 4.521377, rw_size -nan, m 3.626004, 90 5.575364, 99 23.463593
succs ratio 0.998434
[bank_worker.h:126] worker exit.
[0m[bench_listener2.cc:295] Medium latency 2.71299ms, 90th latency 4.30752ms, 99th latency 18.8782ms; average latency: 3.29334ms.
[0m[bank_worker.h:129] read time: 0ms
[0m[occ_statistics.h:63] read_lat time: 0ms
[0m[occ_statistics.h:64] lock time: 0.538615ms
[0m[occ_statistics.h:65] release_write time: 0.170716ms
[0m[occ_statistics.h:66] release_read time: 0ms
[0m[occ_statistics.h:67] renew_lease time: 0ms
[0m[occ_statistics.h:68] validate time: 0ms
[0m[occ_statistics.h:69] log time: 1.04162ms
[0m[occ_statistics.h:70] 2pc time: 0ms
[0m[occ_statistics.h:71] commit time: 0.489613ms
[0m[occ_statistics.h:73] 0
[0m[occ_statistics.h:74] 0.538615
[0m[occ_statistics.h:75] 0.170716
[0m[occ_statistics.h:76] 0
[0m[occ_statistics.h:77] 0.489613
[0m[occ_statistics.h:80] temp time: 0ms
[0m[occ_statistics.h:97] read cpu time: 0ms
[0m[occ_statistics.h:98] lock cpu time: 4.19664e+09ms
[0m[occ_statistics.h:99] release_write cpu time: 7.70393e+09ms
[0m[occ_statistics.h:100] release_read cpu time: 0ms
[0m[occ_statistics.h:101] renew_lease cpu time: 0ms
[0m[occ_statistics.h:102] validate cpu time: 0ms
[0m[occ_statistics.h:103] log cpu time: 2.01216e+10ms
[0m[occ_statistics.h:104] 2pc cpu time: 0ms
[0m[occ_statistics.h:105] commit cpu time: 7.82556e-06ms
[0m[occ_statistics.h:106] temp time: 0ms
[0m[bench_worker.cc:392] 0: 0
[0m[bench_worker.cc:392] 1: 0
[0m[bench_worker.cc:392] 2: 7
[0m[bench_worker.cc:392] 3: 0
[0m[bench_worker.cc:392] 4: 14
[0m[bench_worker.cc:392] 5: 0
[0m[bench_worker.cc:392] 6: 0
[0m[bench_worker.cc:392] 7: 0
[0m[bench_worker.cc:392] 8: 0
[0m[bench_worker.cc:392] 9: 0
[0m[bench_worker.cc:392] 10: 0
[0m[bench_worker.cc:392] 11: 0
[0m[bench_worker.cc:392] 12: 0
[0m[bench_worker.cc:392] 13: 0
[0m[bench_worker.cc:392] 14: 0
[0m[bench_worker.cc:392] 15: 0
[0m[bench_worker.cc:392] 16: 0
[0m[bench_worker.cc:392] 17: 0
[0m[bench_worker.cc:392] 18: 84341
[0m[bench_worker.cc:392] 19: 7740
[0m[bench_worker.cc:392] 20: 0
[0m[bench_worker.cc:392] 21: 0
[0m[bench_worker.cc:392] 22: 0
[0m[bench_worker.cc:392] 23: 0
[0m[bench_worker.cc:392] 24: 0
[0m[bench_worker.cc:392] 25: 0
[0m[bench_worker.cc:392] 26: 15304
[0m[bench_worker.cc:392] 27: 0
[0m[bench_worker.cc:392] 28: 0
[0m[bench_worker.cc:392] 29: 0
[0m[bench_worker.cc:392] 30: 0
[0m[bench_worker.cc:392] 31: 0
[0m[bench_worker.cc:392] 32: 0
[0m[bench_worker.cc:392] 33: 0
[0m[bench_worker.cc:392] 34: 0
[0m[bench_worker.cc:392] 35: 0
[0m[bench_worker.cc:392] 36: 0
[0m[bench_worker.cc:392] 37: 0
[0m[bench_worker.cc:392] 38: 0
[0m[bench_worker.cc:392] 39: 0
[0mmaster routine exit...
[bench_listener2.cc:307] benchmark ends
[0m[bench_runner.cc:313] main runner ends.
[0m